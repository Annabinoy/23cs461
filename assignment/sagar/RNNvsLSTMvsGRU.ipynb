{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent Neural Network: Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['science', 'notebook', 'grid'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Synthetic Data\n",
    "\n",
    "def generate_time_series(n_points=10000):\n",
    "    time = np.linspace(0, 10, n_points)\n",
    "    signal = np.sin(time) \n",
    "    return time, signal\n",
    "\n",
    "time, signal = generate_time_series()\n",
    "\n",
    "# Splitting  data into train and test sets\n",
    "train_size = int(0.8 * len(time))\n",
    "train_time, test_time = time[:train_size], time[train_size:]\n",
    "train_signal, test_signal = signal[:train_size], signal[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Models in Pytorch\n",
    "\n",
    "While defining a RNN object we should take care of two essential parameters:\n",
    "\n",
    "`input_size` — The number of expected features in the input x\n",
    "\n",
    "`hidden_size` — The number of features in the hidden state h\n",
    "\n",
    "The input_size is 1 since we are using one time step of each sequence. Also, we can use `n_layers` parameter to get a stacked RNN with n hidden layers.\n",
    "\n",
    "Note: if `batch_first` = True, then batch dimension in input and output comes first.\n",
    "\n",
    "#### Inputs\n",
    "\n",
    "1. input shape (batch, seq_len, input_size): tensor containing the features of the input sequence.\n",
    "\n",
    "2. h_0 of shape (batch, num_layers * num_directions,  hidden_size): tensor containing the initial hidden state for each element in the batch.\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "1. output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n",
    "\n",
    "2. h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_signal, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_signal, dtype=torch.float32)\n",
    "signal_data = torch.tensor(signal, dtype=torch.float32)\n",
    "\n",
    "# hyperparameters and model instances\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "rnn_model = RNNModel(input_size, hidden_size, output_size)\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "gru_optimizer = torch.optim.Adam(gru_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], RNN Loss: 0.0001, LSTM Loss: 0.5074, GRU Loss: 0.2691\n",
      "Epoch [20/100], RNN Loss: 0.0018, LSTM Loss: 0.0069, GRU Loss: 0.0063\n",
      "Epoch [30/100], RNN Loss: 0.0051, LSTM Loss: 0.0050, GRU Loss: 0.0001\n",
      "Epoch [40/100], RNN Loss: 0.0042, LSTM Loss: 0.0047, GRU Loss: 0.0070\n",
      "Epoch [50/100], RNN Loss: 0.0019, LSTM Loss: 0.0039, GRU Loss: 0.0004\n",
      "Epoch [60/100], RNN Loss: 0.0005, LSTM Loss: 0.0010, GRU Loss: 0.0003\n",
      "Epoch [70/100], RNN Loss: 0.0001, LSTM Loss: 0.0000, GRU Loss: 0.0003\n",
      "Epoch [80/100], RNN Loss: 0.0000, LSTM Loss: 0.0000, GRU Loss: 0.0000\n",
      "Epoch [90/100], RNN Loss: 0.0000, LSTM Loss: 0.0000, GRU Loss: 0.0000\n",
      "Epoch [100/100], RNN Loss: 0.0000, LSTM Loss: 0.0000, GRU Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    rnn_optimizer.zero_grad()\n",
    "    lstm_optimizer.zero_grad()\n",
    "    gru_optimizer.zero_grad()\n",
    "\n",
    "    rnn_output = rnn_model(train_data.unsqueeze(0).unsqueeze(2))\n",
    "    lstm_output = lstm_model(train_data.unsqueeze(0).unsqueeze(2))\n",
    "    gru_output = gru_model(train_data.unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "    rnn_target = train_data[-1].view(1, 1)\n",
    "    lstm_target = train_data[-1].view(1, 1)\n",
    "    gru_target = train_data[-1].view(1, 1)\n",
    "\n",
    "    rnn_loss = criterion(rnn_output, rnn_target)\n",
    "    lstm_loss = criterion(lstm_output, lstm_target)\n",
    "    gru_loss = criterion(gru_output, gru_target)\n",
    "\n",
    "    rnn_loss.backward()\n",
    "    lstm_loss.backward()\n",
    "    gru_loss.backward()\n",
    "\n",
    "    rnn_optimizer.step()\n",
    "    lstm_optimizer.step()\n",
    "    gru_optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], RNN Loss: {rnn_loss.item():.4f}, LSTM Loss: {lstm_loss.item():.4f}, GRU Loss: {gru_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Prediction: 0.5249654650688171\n",
      "LSTM Prediction: 0.5862373113632202\n",
      "GRU Prediction: 0.3224380910396576\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    rnn_model.eval()\n",
    "    lstm_model.eval()\n",
    "    gru_model.eval()\n",
    "\n",
    "    rnn_test_output = rnn_model(test_data.unsqueeze(0).unsqueeze(2))\n",
    "    lstm_test_output = lstm_model(test_data.unsqueeze(0).unsqueeze(2))\n",
    "    gru_test_output = gru_model(test_data.unsqueeze(0).unsqueeze(2))\n",
    "\n",
    "print(f'RNN Prediction: {rnn_test_output.item()}')\n",
    "print(f'LSTM Prediction: {lstm_test_output.item()}')\n",
    "print(f'GRU Prediction: {gru_test_output.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, signal, label='Original Time Series', color='blue')\n",
    "\n",
    "# Predictions made by RNN, LSTM, and GRU models at their respective time points\n",
    "with torch.no_grad():\n",
    "    rnn_preds = [rnn_model(signal_data[:i+1].unsqueeze(0).unsqueeze(2)).item() for i in range(len(time))]\n",
    "    lstm_preds = [lstm_model(signal_data[:i+1].unsqueeze(0).unsqueeze(2)).item() for i in range(len(time))]\n",
    "    gru_preds = [gru_model(signal_data[:i+1].unsqueeze(0).unsqueeze(2)).item() for i in range(len(time))]\n",
    "\n",
    "plt.scatter(time, rnn_preds, label='RNN Prediction', marker='o', color='red')\n",
    "plt.scatter(time, lstm_preds, label='LSTM Prediction', marker='x', color='green')\n",
    "plt.scatter(time, gru_preds, label='GRU Prediction', marker='^', color='purple')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Signal')\n",
    "plt.legend()\n",
    "plt.title('Original Time Series and Model Predictions')\n",
    "plt.savefig('without_random_noise.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
